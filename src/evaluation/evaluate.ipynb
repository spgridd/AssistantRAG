{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d7b46da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30982503",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "from llm.chain import get_conversation_chain\n",
    "from wrappers.langchain_wrappers import VertexAIChat, VertexAIEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0e2543aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "normal = pd.read_csv(\"../data/RAG_evaluation_dataset.csv\")\n",
    "\n",
    "shorter = normal[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9304a12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "shorter.to_csv(\"../data/RAG_evaluation_dataset_shorter.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b8455577",
   "metadata": {},
   "outputs": [],
   "source": [
    "FAISS_INDEX_DIR = \"../data/faiss_index_with_images\"\n",
    "EVAL_CSV_DIR = \"../data/RAG_evaluation_dataset_shorter.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d7e4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import google.genai as genai\n",
    "import os\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "PROJECT_ID = os.getenv(\"GEMINI_PROJECT\")\n",
    "LOCATION = os.getenv(\"GEMINI_LOCATION\")\n",
    "USE_VERTEXAI = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "af85ed9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas import SingleTurnSample\n",
    "from ragas.metrics import Faithfulness, LLMContextPrecisionWithReference, ResponseRelevancy\n",
    "\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "\n",
    "evaluator_llm = LangchainLLMWrapper(VertexAIChat(model=\"gemini-2.0-flash\", temperature=0.0))\n",
    "\n",
    "evaluator_embeddings = LangchainEmbeddingsWrapper(VertexAIEmbedding())\n",
    "\n",
    "FAISS_INDEX_PATH = \"../data/faiss_index_with_images\"\n",
    "\n",
    "embedding_model = VertexAIEmbedding()\n",
    "vector_store = FAISS.load_local(FAISS_INDEX_PATH, embedding_model, allow_dangerous_deserialization=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93f734e",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_f = 0\n",
    "all_f = []\n",
    "avg_c = 0\n",
    "all_c = []\n",
    "avg_r = 0\n",
    "all_r = []\n",
    "\n",
    "for i in range(len(normal)):\n",
    "    question = normal.iloc[i]['Question']\n",
    "    reference = normal.iloc[i]['Ground_Truth_Context']\n",
    "\n",
    "    chain, retriever = get_conversation_chain(\n",
    "        vector_store=vector_store,\n",
    "        re_ranker=False,\n",
    "        faiss=True,\n",
    "        user_prompt=question\n",
    "    )\n",
    "\n",
    "    response = chain({\"question\": question})\n",
    "\n",
    "    retrieved_docs = retriever.get_relevant_documents(question)\n",
    "\n",
    "    retrieved_contexts = []\n",
    "    for i, doc in enumerate(retrieved_docs, 1):\n",
    "        metadata = doc.metadata\n",
    "        retrieved_contexts.append(doc.page_content)\n",
    "\n",
    "    test_data = {\n",
    "        \"user_input\": question,\n",
    "        \"retrieved_contexts\": retrieved_contexts,\n",
    "        \"response\": response['answer'],\n",
    "        \"reference\": reference,\n",
    "    }\n",
    "\n",
    "    sample_f = SingleTurnSample(\n",
    "        user_input=test_data[\"user_input\"],\n",
    "        retrieved_contexts=test_data['retrieved_contexts'],\n",
    "        response=test_data['response'],\n",
    "    )\n",
    "    sample_c = SingleTurnSample(\n",
    "        user_input=test_data[\"user_input\"],\n",
    "        reference=test_data[\"reference\"],\n",
    "        retrieved_contexts=test_data['retrieved_contexts'],\n",
    "    )\n",
    "\n",
    "    faithfulness_metric = Faithfulness(llm=evaluator_llm)\n",
    "    context_precision = LLMContextPrecisionWithReference(llm=evaluator_llm)\n",
    "    scorer = ResponseRelevancy(llm=evaluator_llm, embeddings=evaluator_embeddings)\n",
    "\n",
    "    faithfulness_score = await faithfulness_metric.single_turn_ascore(sample_f)\n",
    "    context_precision_score = await context_precision.single_turn_ascore(sample_c)\n",
    "    response_relevance_score = await scorer.single_turn_ascore(sample_f)\n",
    "\n",
    "    all_f.append(faithfulness_score)\n",
    "    avg_f += faithfulness_score\n",
    "\n",
    "    all_c.append(context_precision_score)\n",
    "    avg_c += context_precision_score\n",
    "\n",
    "    all_r.append(response_relevance_score)\n",
    "    avg_r += response_relevance_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f1c35eec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Faithfullness = 0.7518907563025212\n",
      "Avg Context Precision = 0.5770003988200691\n",
      "Avg Response Relevance = 0.8534368604970144\n"
     ]
    }
   ],
   "source": [
    "print(f\"Avg Faithfullness = {avg_f / len(normal)}\")\n",
    "print(f\"Avg Context Precision = {avg_c / len(normal)}\")\n",
    "print(f\"Avg Response Relevance = {avg_r / len(normal)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
